{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "6IPtXjCX9UMo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer configuration\n",
        "tokenizer_config_path = '/content/drive/MyDrive/daa/tokenizer_config.npy'\n",
        "tokenizer_config = np.load(tokenizer_config_path, allow_pickle=True).item()\n",
        "\n",
        "# Create a tokenizer with the loaded configuration\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.word_index = tokenizer_config['word_index']\n",
        "tokenizer.document_count = tokenizer_config['document_count']\n",
        "\n",
        "# Load the saved model\n",
        "model_path = '/content/drive/MyDrive/daa/my_poem_model'\n",
        "loaded_model = tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "1z3ibHf73ghp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text for prediction\n",
        "example_text = input()\n",
        "\n",
        "# Tokenize and pad the example text\n",
        "example_seq = tokenizer.texts_to_sequences([example_text])\n",
        "example_pad = pad_sequences(example_seq, maxlen=200)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "prediction = loaded_model.predict(example_pad)\n",
        "\n",
        "if prediction[0,0] > 0.5:\n",
        "    print(f\"예측 결과: 인간의 시로 판단됩니다. (확률: {prediction[0,0]:.4f})\")\n",
        "else:\n",
        "    print(f\"예측 결과: AI가 생성한 시로 판단됩니다. (확률: {1 - prediction[0,0]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ2sa6t534GZ",
        "outputId": "9aa4a6da-f8c9-4770-ea6e-aef520924824"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "왜 나온거니  안 불렀는데\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "예측 결과: 인간의 시로 판단됩니다. (확률: 0.7315)\n"
          ]
        }
      ]
    }
  ]
}